{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Weaviate RAG with Arize Hallucination Evaluation\n",
        "\n",
        "This notebook demonstrates:\n",
        "1. Loading E-Commerce FAQ data from HuggingFace into Weaviate\n",
        "2. Creating a RAG system to answer user questions\n",
        "3. Using Arize hallucination evaluator to check answer quality\n",
        "4. Implementing a feedback loop to retry with anti-hallucination prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "%pip install -q arize-phoenix-evals openai weaviate-client datasets huggingface-hub nest-asyncio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import weaviate\n",
        "import nest_asyncio\n",
        "from datasets import load_dataset\n",
        "from dotenv import load_dotenv\n",
        "from phoenix.evals import llm_classify, OpenAIModel\n",
        "from openai import OpenAI\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "\n",
        "load_dotenv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "url = os.environ.get(\"WEAVIATE_URL\")\n",
        "api_key = os.environ.get(\"WEAVIATE_API_KEY\")\n",
        "openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "aws_access_key = os.environ.get(\"AWS_ACCESS_KEY\")\n",
        "aws_secret_key = os.environ.get(\"AWS_SECRET_KEY\")\n",
        "\n",
        "weaviate_client = weaviate.connect_to_weaviate_cloud(\n",
        "    cluster_url=url,\n",
        "    auth_credentials=weaviate.auth.AuthApiKey(api_key),\n",
        "    headers={\n",
        "        \"X-OpenAI-Api-Key\": openai_key,\n",
        "        \"X-AWS-Access-Key\": aws_access_key,\n",
        "        \"X-AWS-Secret-Key\": aws_secret_key,\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weaviate.classes.config import Configure, Property, DataType\n",
        "\n",
        "collection_name = \"EcommerceFAQ\"\n",
        "\n",
        "try:\n",
        "   weaviate_client.collections.delete(collection_name)\n",
        "   print(f\"Deleted existing collection: {collection_name}\")\n",
        "except:\n",
        "   print(f\"Collection {collection_name} doesn't exist, creating new one\")\n",
        "\n",
        "faq_collection = weaviate_client.collections.create(\n",
        "   name=collection_name,\n",
        "   properties=[\n",
        "      Property(name=\"question\", data_type=DataType.TEXT),\n",
        "      Property(name=\"answer\", data_type=DataType.TEXT),\n",
        "      Property(name=\"category\", data_type=DataType.TEXT),\n",
        "      Property(name=\"parent_category\", data_type=DataType.TEXT),\n",
        "      Property(name=\"question_id\", data_type=DataType.TEXT),\n",
        "      Property(name=\"category_id\", data_type=DataType.TEXT),\n",
        "   ],\n",
        "   vectorizer_config=Configure.Vectorizer.text2vec_openai(\n",
        "      model=\"text-embedding-3-small\"\n",
        "   ),\n",
        "   generative_config=Configure.Generative.aws(\n",
        "      region=\"eu-west-2\",\n",
        "      service=\"bedrock\",                     \n",
        "      model=\"amazon.titan-text-express-v1\"\n",
        "   )\n",
        ")\n",
        "print(f\"Created collection: {collection_name}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\" Loading E-Commerce FAQ dataset...\")\n",
        "dataset = load_dataset(\"NebulaByte/E-Commerce_FAQs\")\n",
        "faq_data = dataset['train'].to_pandas()\n",
        "\n",
        "print(f\"Dataset loaded: {len(faq_data)} FAQ entries\")\n",
        "print(\"Dataset columns:\", faq_data.columns.tolist())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Importing FAQ data...\")\n",
        "with weaviate_client.batch.dynamic() as batch:\n",
        "    for idx, row in faq_data.iterrows():\n",
        "        batch.add_object(\n",
        "            collection=collection_name,\n",
        "            properties={\n",
        "                \"question\": row[\"question\"],\n",
        "                \"answer\": row[\"answer\"],\n",
        "                \"category\": row[\"category\"],\n",
        "                \"parent_category\": row[\"parent_category\"],\n",
        "                \"question_id\": str(row[\"question_id\"]),\n",
        "                \"category_id\": str(row[\"category_id\"])\n",
        "            }\n",
        "        )\n",
        "\n",
        "print(f\"Imported {len(faq_data)} FAQ entries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Core Weaviate RAG function - returns only essential data needed for evaluation\n",
        "    \n",
        "    Args:\n",
        "        question: User's question for search\n",
        "        generative_prompt: The complete prompt template for generation\n",
        "        limit: Number of search results to retrieve\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with answer and context for evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from weaviate.classes.config import Configure\n",
        "from weaviate.classes.generate import GenerativeConfig\n",
        "\n",
        "def weaviate_rag(question: str, generative_prompt: str, limit: int = 3) -> Dict:\n",
        "    try:\n",
        "        response = faq_collection.generate.hybrid(\n",
        "            query=question,\n",
        "            limit=limit,\n",
        "            grouped_task=generative_prompt,\n",
        "            return_metadata=[\"score\"]\n",
        "        )\n",
        "        \n",
        "        context_items = []\n",
        "        for obj in response.objects:\n",
        "            context_items.append(\n",
        "                f\"Q: {obj.properties['question']}\\n\"\n",
        "                f\"A: {obj.properties['answer']}\\n\"\n",
        "                f\"Category: {obj.properties['category']}\"\n",
        "            )\n",
        "\n",
        "        context = \"\\n\\n---\\n\\n\".join(context_items)\n",
        "        \n",
        "        answer = response.generated if response.generated else \"\"\n",
        "        return {\n",
        "            \"answer\": answer,\n",
        "            \"context\": context\n",
        "        }      \n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"answer\": f\"Error occurred: {str(e)}\",\n",
        "            \"context\": \"\"\n",
        "        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluate if the answer contains hallucinated information using llm_classify\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        answer: RAG system's answer\n",
        "        context: Context used to generate the answer\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with evaluation results from llm_classify"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "RAG_HALLUCINATION_TEMPLATE = '''\n",
        "You are evaluating whether an AI assistant's response contains hallucinated information when answering a question based on provided context.\n",
        "\n",
        "[BEGIN DATA]\n",
        "************\n",
        "[Question]: {question}\n",
        "************\n",
        "[Context]: {context}\n",
        "************\n",
        "[Response]: {answer}\n",
        "[END DATA]\n",
        "\n",
        "Evaluate if the response contains information that is NOT supported by the provided context.\n",
        "\n",
        "Hallucination occurs when:\n",
        "- The response includes facts, numbers, or details not found in the context\n",
        "- The response makes claims that contradict the context\n",
        "- The response invents specific information not mentioned in the context\n",
        "\n",
        "NOT hallucination when:\n",
        "- The response says it doesn't have enough information\n",
        "- The response only uses information from the context\n",
        "- The response makes reasonable inferences clearly based on the context\n",
        "\n",
        "Your answer must be a single word: \"hallucinated\" or \"factual\".\n",
        "'''\n",
        "\n",
        "eval_judge_model = OpenAIModel(\n",
        "    model=\"gpt-4o\", temperature=0, api_key=openai_key\n",
        ")\n",
        "\n",
        "def evaluate_hallucination_in_RAG_response(question: str, answer: str, context: str) -> Dict:\n",
        "    \n",
        "    user_query_df = pd.DataFrame({\n",
        "        \"question\": [question],\n",
        "        \"answer\": [answer],\n",
        "        \"context\": [context]\n",
        "    })\n",
        "    \n",
        "    eval_result = llm_classify(\n",
        "        data=user_query_df,\n",
        "        template=RAG_HALLUCINATION_TEMPLATE,\n",
        "        model=eval_judge_model,\n",
        "        rails=[\"hallucinated\", \"factual\"],\n",
        "        provide_explanation=True\n",
        "    )\n",
        "    \n",
        "    result = eval_result.iloc[0].to_dict()\n",
        "    \n",
        "    if 'explanation' in result:\n",
        "        print(f\"  Explanation: {result['explanation']}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"Hallucination evaluator defined (using llm_classify)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Main RAG workflow with hallucination detection and retry\n",
        "    \n",
        "    Args:\n",
        "        question: User's question\n",
        "        max_retries: Maximum retries if hallucination detected\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with final answer and all attempt details"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def complete_rag_with_hallucination_check(question: str, max_retries: int = 1) -> Dict:\n",
        "    attempts = []\n",
        "    \n",
        "    base_prompt = \"\"\"\n",
        "    You are a helpful e-commerce customer service assistant. Answer the user's question based ONLY on the provided context from the FAQ database.\n",
        "    \n",
        "    IMPORTANT RULES:\n",
        "    - Only use information from the provided FAQ context\n",
        "    - Be concise and helpful\n",
        "    - Don't make up information that's not in the context\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"Attempt 1:\")\n",
        "    \n",
        "    # First attempt with base prompt\n",
        "    generative_prompt = f\"{base_prompt}\\n\\nUser Question: {{{{ question }}}}\\n\\nBased on the context provided, please answer the question:\"\n",
        "    weaviate_result = weaviate_rag(question, generative_prompt)\n",
        "    \n",
        "    rag_result = {\n",
        "        \"question\": question,\n",
        "        \"answer\": weaviate_result[\"answer\"],\n",
        "        \"context\": weaviate_result[\"context\"]\n",
        "    }\n",
        "    \n",
        "    print(f\"Generated answer: {rag_result['answer']}\")\n",
        "    \n",
        "    print(\"Evaluating for hallucination...\")\n",
        "    eval_result = evaluate_hallucination_in_RAG_response(\n",
        "        question=rag_result[\"question\"],\n",
        "        answer=rag_result[\"answer\"],\n",
        "        context=rag_result[\"context\"]\n",
        "    )\n",
        "    \n",
        "    attempts.append({\n",
        "        \"attempt\": 1,\n",
        "        \"rag_result\": rag_result,\n",
        "        \"eval_result\": eval_result\n",
        "    })\n",
        "    \n",
        "    print(f\"Evaluation result: {eval_result['label']}\")\n",
        "    \n",
        "    # Retry loop if hallucination detected\n",
        "    retry_count = 0\n",
        "    while eval_result[\"label\"] == \"hallucinated\" and retry_count < max_retries:\n",
        "        retry_count += 1\n",
        "        print(f\"\\nHallucination detected! Retrying (Attempt {retry_count + 1})...\")\n",
        "        \n",
        "        # Add anti-hallucination instruction to base prompt\n",
        "        retry_prompt = base_prompt + \"\"\"\n",
        "        \n",
        "        CRITICAL WARNING:\n",
        "        Your previous response was flagged as potentially containing hallucinated information.\n",
        "        Be EXTREMELY careful to only use information explicitly stated in the provided context.\n",
        "        If you cannot find the answer in the context, clearly state that you don't have enough information.\n",
        "        Do NOT add any information that is not directly mentioned in the context.\n",
        "        \"\"\"\n",
        "        \n",
        "        # Retry with enhanced prompt\n",
        "        generative_prompt = f\"{retry_prompt}\\n\\nUser Question: {{{{ question }}}}\\n\\nBased on the context provided, please answer the question:\"\n",
        "        weaviate_result = weaviate_rag(question, generative_prompt)\n",
        "        \n",
        "        rag_result = {\n",
        "            \"question\": question,\n",
        "            \"answer\": weaviate_result[\"answer\"],\n",
        "            \"context\": weaviate_result[\"context\"]\n",
        "        }\n",
        "        \n",
        "        print(f\"New answer: {rag_result['answer']}\")\n",
        "        \n",
        "        print(\"Re-evaluating for hallucination...\")\n",
        "        eval_result = evaluate_hallucination_in_RAG_response(\n",
        "            question=rag_result[\"question\"],\n",
        "            answer=rag_result[\"answer\"],\n",
        "            context=rag_result[\"context\"]\n",
        "        )\n",
        "        \n",
        "        attempts.append({\n",
        "            \"attempt\": retry_count + 1,\n",
        "            \"rag_result\": rag_result,\n",
        "            \"eval_result\": eval_result\n",
        "        })\n",
        "        \n",
        "        print(f\"Re-evaluation result: {eval_result['label']}\")\n",
        "    \n",
        "    final_result = {\n",
        "        \"question\": question,\n",
        "        \"final_answer\": rag_result[\"answer\"],\n",
        "        \"final_evaluation\": eval_result,\n",
        "        \"total_attempts\": len(attempts),\n",
        "        \"hallucination_resolved\": not eval_result[\"label\"] == \"hallucinated\",\n",
        "        \"all_attempts\": attempts\n",
        "    }\n",
        "    \n",
        "    if eval_result[\"label\"] == \"hallucinated\":\n",
        "        print(f\"\\nFinal result: Hallucination still detected after {max_retries} retries\")\n",
        "    else:\n",
        "        print(f\"\\nFinal result: Answer is factual (resolved in {len(attempts)} attempt{'s' if len(attempts) > 1 else ''})\")\n",
        "    \n",
        "    return final_result\n",
        "\n",
        "print(\"Complete RAG workflow with hallucination check defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "test_questions = [\n",
        "    \"What should I do if I missed my delivery?\",\n",
        "    \"Can I use my saved cards on mobile app?\",\n",
        "    \"How do I return a product?\",\n",
        "    \"What is your refund policy for electronics?\",\n",
        "    \"Can you tell me about your space program?\"  # Edge case: completely irrelevant\n",
        "]\n",
        "\n",
        "print(\"Testing RAG workflow with sample questions\\n\")\n",
        "\n",
        "test_results = []\n",
        "for i, question in enumerate(test_questions, 1):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Question {i}/{len(test_questions)}: {question}\")\n",
        "    print('='*60)\n",
        "    \n",
        "    result = complete_rag_with_hallucination_check(question)\n",
        "    test_results.append(result)\n",
        "    \n",
        "    print(f\"\\nFinal Answer: {result['final_answer']}\")\n",
        "    print(f\"Factual: {result['final_evaluation']['label'] == 'factual'}\")\n",
        "    print(f\"Total Attempts: {result['total_attempts']}\")\n",
        "    \n",
        "    time.sleep(1)\n",
        "\n",
        "print(f\"\\n\\nCompleted {len(test_results)} tests\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cleanup\n",
        "try:\n",
        "    weaviate_client.close()\n",
        "    print(\"Weaviate connection closed\")\n",
        "except:\n",
        "    print(\"Connection already closed\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
